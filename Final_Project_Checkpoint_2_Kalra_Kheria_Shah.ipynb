{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **CRASH ANALYTICS FOR MONTGOMERY COUNTY**\n",
        "\n",
        "## Team: Tanay Shah, Ishaan Kalra, Aadesh Kheria\n",
        "\n",
        "### **Project Question🤨->** \n",
        "\n",
        "**Project Brief** \n",
        "\n",
        "The primary goal of this project is to conduct a comprehensive analysis of traffic crash data across Montgomery County from 2015 to 2023. \n",
        "\n",
        "By examining monthly trends over these years, we aim to provide the UMD community and the general public with insights into the most prevalent types of traffic incidents, as well as the locations and times they most frequently occur. \n",
        "\n",
        "Throughout this tutorial, we will explore potential correlations between the types of crashes and their specific locations, the frequency of these incidents at different hours, and how these patterns might vary with the seasons. Our objective is to uncover meaningful trends that can inform safety improvements and awareness initiatives within the community.\n",
        "\n",
        "**Required Tools**\n",
        "You will need to install the following librairies for this project:\n",
        "- pandas\n",
        "- numpy\n",
        "- matplotlib\n",
        "- seaborn\n",
        "- scikit-learn\n",
        "- scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -q pandas\n",
        "%pip install -q numpy\n",
        "%pip install -q matplotlib\n",
        "%pip install -q seaborn\n",
        "%pip install -q scikit-learn\n",
        "%pip install -q scipy\n",
        "%pip install -q statsmodels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XblzRx5LzKqc"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from statsmodels.stats.proportion import proportions_ztest\n",
        "import seaborn as sns\n",
        "from scipy.stats import ttest_ind, chi2_contingency\n",
        "import pandas as pd\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Dataset Collection & Preprocessing**\n",
        "\n",
        "**1. DATA COLLECTION:**\n",
        "\n",
        "This is the initial stage of our data lifecycle, focusing on the systematic collection of data from established datasets or various files.\n",
        "\n",
        "In this project, we sourced our data directly from the Montgomery County Government's open data portal. The specific dataset used, titled \"Crash Reporting Incidents Data,\" is available for direct download at [https://data.montgomerycountymd.gov/Public-Safety/Crash-Reporting-Incidents-Data/bhju-22kf/about_data](https://data.montgomerycountymd.gov/Public-Safety/Crash-Reporting-Incidents-Data/bhju-22kf/about_data). This comprehensive dataset contains detailed records of traffic incidents reported in Montgomery County, which serves as a foundational element for our analysis.\n",
        "\n",
        "For managing and manipulating the downloaded CSV file, we utilized the following tools:\n",
        "\n",
        "- `pandas`: for robust data manipulation and analysis.\n",
        "- `numpy`: for high-level mathematical functions and operations on arrays.\n",
        "- `datetime`: for handling date and time data.\n",
        "- `os.path`: for managing file paths.\n",
        "\n",
        "Our process involves downloading the CSV file directly from the provided link, which ensures that we have the complete and up-to-date dataset. We then load this data into a pandas DataFrame, enabling us to perform detailed analyses and manipulations as required by our project’s objectives.\n",
        "\n",
        "Below, we will detail the steps involved in processing the data to demonstrate how we prepare and analyze the data for our needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGcF6CvGzjHW"
      },
      "outputs": [],
      "source": [
        "crash_df = pd.read_csv('crash.csv')\n",
        "crash_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYqeDD750aXa"
      },
      "outputs": [],
      "source": [
        "rows, cols = crash_df.shape\n",
        "print(\"Rows: \" + str(rows) + \"\\n\" + \"Columns: \" + str(cols))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Qske67c08Hd"
      },
      "outputs": [],
      "source": [
        "for col_names in crash_df.columns:\n",
        "  print(col_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find the earliest date\n",
        "earliest_date = crash_df['Crash Date/Time'].min()\n",
        "\n",
        "# Find the latest date\n",
        "latest_date = crash_df['Crash Date/Time'].max()\n",
        "\n",
        "# Print the results\n",
        "print(\"Earliest crash date:\", earliest_date)\n",
        "print(\"Latest crash date:\", latest_date)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**2. DATA PREPROCESSING:**\n",
        "\n",
        "This phase is crucial for preparing the raw data into a format suitable for analysis. Given the extensive nature of the dataset and our specific research objectives, we adopted a selective approach to data preprocessing to ensure efficiency and relevance in our analysis.\n",
        "\n",
        "### Column Selection:\n",
        "Initially, we identified and retained only the most pertinent columns from the complete dataset. This was done to reduce computational load and focus on the data most relevant to our research questions. The selected columns include identifiers like 'Report Number' and 'Local Case Number', details about the crash such as 'Crash Date/Time', 'Collision Type', and environmental conditions like 'Weather' and 'Light'. \n",
        "\n",
        "### Data Cleaning:\n",
        "The following steps were taken to clean and prepare the data:\n",
        "\n",
        "- **Conversion to Suitable Data Types**: We converted the 'Crash Date/Time' column into a datetime format, allowing us to extract and analyze the date and time separately.\n",
        "- **Handling Missing Values**: We filled missing values in categorical columns like 'Hit/Run' and 'Route Type' with the most frequent values or categorized them as 'Unknown'. Numeric fields such as 'Mile Point' were filled using median values to maintain data integrity.\n",
        "- **Imputation Techniques**: For more complex fields like 'Traffic Control' and 'Light', where data might depend on other factors such as 'Road Name' or the time of the crash, we used group-specific modes and conditional imputation. This approach helps maintain logical consistency across related data fields.\n",
        "- **Normalization and Error Correction**: We also addressed duplicates and corrected erroneous entries to ensure the uniqueness and accuracy of our data records.\n",
        "\n",
        "### Enhancing Data Quality:\n",
        "To further enhance the quality of our dataset, we employed several strategies:\n",
        "- **Mode Imputation for Categorical Data**: We imputed missing values in 'Weather' and 'Collision Type' using the most common values conditioned on related attributes, ensuring that our data reflects realistic and probable scenarios.\n",
        "- **Custom Functions for Data Imputation**: For attributes like 'Traffic Control' and 'Light', custom functions were developed to impute values based on the derived rules from the data itself. This method provided a nuanced approach to handling missing data, particularly in cases where simple imputation might not suffice.\n",
        "\n",
        "### Final Dataset Preparation:\n",
        "After these steps, the dataset was transformed into a cleaner, more manageable format, making it ready for the subsequent stages of data analysis and visualization. The focus on careful preprocessing ensures that our findings are based on accurate and representative data, thereby enhancing the reliability of our conclusions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "js-lhcdH2lQ6"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "For the sake of precise analysis and considering the large file size,\n",
        "we are only considering columns that seem essential to our analysis,\n",
        "whilst striping the rest.\n",
        "\n",
        "If this is not possible, our team would be happy to fix this and consider the\n",
        "entire dataset.\n",
        "'''\n",
        "columns_to_keep = [\n",
        "    'Report Number', 'Local Case Number', 'Agency Name', 'ACRS Report Type',\n",
        "    'Crash Date/Time', 'Hit/Run', 'Route Type', 'Mile Point', 'Lane Direction',\n",
        "    'Lane Number', 'Number of Lanes', 'Direction', 'Road Name', 'Cross-Street Name',\n",
        "    'At Fault', 'Collision Type', 'Weather', 'Surface Condition', 'Light', 'Traffic Control', 'Latitude', 'Longitude', 'At Fault'\n",
        "]\n",
        "\n",
        "filtered_df = crash_df[columns_to_keep]\n",
        "filtered_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaT8e1zB3N09",
        "outputId": "0cda97f6-29dc-4acc-a297-c2ee8f6f8ee9"
      },
      "outputs": [],
      "source": [
        "filtered_df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNJKETgW3fiT"
      },
      "outputs": [],
      "source": [
        "info = filtered_df.count()\n",
        "info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgbieYOm68qy"
      },
      "outputs": [],
      "source": [
        "null_counts_corrected = filtered_df.isnull().sum()\n",
        "null_counts_corrected"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf8BLK1H3xp_"
      },
      "source": [
        "This indicates there are numerous null values in multiple columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvysDGif3urr"
      },
      "outputs": [],
      "source": [
        "print(filtered_df.duplicated().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rl4Tji9_37ct"
      },
      "outputs": [],
      "source": [
        "filtered_df['Crash Date/Time'] = pd.to_datetime(filtered_df['Crash Date/Time'])\n",
        "filtered_df['Crash Date'] = filtered_df['Crash Date/Time'].dt.date\n",
        "filtered_df['Crash Time'] = filtered_df['Crash Date/Time'].dt.time\n",
        "filtered_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22f4fyIz46S3"
      },
      "source": [
        "Convertng dates to ISO format, further making dates and times into two different columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaDX2lq44Jjo"
      },
      "outputs": [],
      "source": [
        "hit_run_mode = filtered_df['Hit/Run'].mode()[0]\n",
        "filtered_df['Hit/Run'] = filtered_df['Hit/Run'].fillna(hit_run_mode)\n",
        "na_hit = filtered_df[filtered_df['Hit/Run'].isna()]\n",
        "na_hit.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwcyuKjrFJGR"
      },
      "source": [
        "Imputated the Hit/Run Column as it consisted of two N/A values. The above shows that the N/A values for the Hit/Run Column do not exist anymore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THjTo0laGxlg"
      },
      "outputs": [],
      "source": [
        "filtered_df['Route Type'] = filtered_df['Route Type'].fillna('Unknown')\n",
        "filtered_df['Route Type'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEAPwU_bJLN5"
      },
      "outputs": [],
      "source": [
        "mile_point_median = filtered_df['Mile Point'].median()\n",
        "print(\"The median for Mile Point's data is: \" + str(mile_point_median))\n",
        "filtered_df['Mile Point'] = filtered_df['Mile Point'].fillna(mile_point_median)\n",
        "filtered_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SfOX140I4xP"
      },
      "source": [
        "Using the median to impute missing values in \"Mile Point\" for robustness against skewed data and outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6O4TWA6J4SC"
      },
      "outputs": [],
      "source": [
        "for i in ['Lane Direction', 'Road Name', 'Cross-Street Name', 'Route Type', 'Direction', 'Surface Condition']:\n",
        "  filtered_df[i] = filtered_df[i].fillna('Unknown')\n",
        "\n",
        "filtered_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CJmGpLlJ43_"
      },
      "source": [
        "Since \"Lane Direction\" indicates the direction of the lane in which the incident occurred, I recognize it as a significant piece of information that can affect the understanding of each crash's context. However, without a clear way to deduce the missing directions from other data (unless there's a discernible pattern or correlation with other variables), my safest approach would be to impute these missing values with a placeholder value such as \"Unknown\". This method acknowledges the missing data without making assumptions that could introduce bias into my analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvgPXKssQSzg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "traffic_control_mapping = filtered_df.groupby('Road Name')['Traffic Control'].agg(lambda x: x.mode()[0] if not x.mode().empty else np.nan).to_dict()\n",
        "\n",
        "def impute_traffic_control(row):\n",
        "    if pd.isna(row['Traffic Control']) and row['Road Name'] in traffic_control_mapping:\n",
        "        return traffic_control_mapping[row['Road Name']]\n",
        "    else:\n",
        "        return row['Traffic Control']\n",
        "\n",
        "filtered_df['Traffic Control'] = filtered_df.apply(impute_traffic_control, axis=1)\n",
        "\n",
        "filtered_df['Traffic Control'] = filtered_df['Traffic Control'].fillna('Unknown')\n",
        "\n",
        "filtered_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9dwsn7RSewk"
      },
      "outputs": [],
      "source": [
        "def impute_light_condition(row):\n",
        "    if pd.isna(row['Light']):\n",
        "        crash_time = row['Crash Time']\n",
        "\n",
        "        daylight_start = datetime.time(6, 0)\n",
        "        daylight_end = datetime.time(18, 0)\n",
        "        dusk_end = datetime.time(19, 0)\n",
        "        dawn_start = datetime.time(5, 0)\n",
        "\n",
        "        if daylight_start <= crash_time <= daylight_end:\n",
        "            return 'DAYLIGHT'\n",
        "        elif daylight_end < crash_time <= dusk_end:\n",
        "            return 'DUSK'\n",
        "        elif dawn_start <= crash_time < daylight_start:\n",
        "            return 'DAWN'\n",
        "        else:\n",
        "\n",
        "            return 'DARK -- UNKNOWN LIGHTING'\n",
        "\n",
        "    else:\n",
        "        return row['Light']\n",
        "\n",
        "\n",
        "if not isinstance(filtered_df['Crash Time'].iloc[0], datetime.time):\n",
        "    filtered_df['Crash Time'] = pd.to_datetime(filtered_df['Crash Time'], format='%H:%M:%S').dt.time\n",
        "\n",
        "filtered_df['Light'] = filtered_df.apply(impute_light_condition, axis=1)\n",
        "filtered_df.head(n=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code first checks if the \"Light\" column is missing a value for a given row. If it is, it then determines the light condition based on the \"Crash Time\" compared to predefined time ranges that represent daylight, dusk, and dawn. For times outside of these ranges, it defaults to \"DARK -- UNKNOWN LIGHTING\" to reflect a conservative approach during nighttime hours. This method helps to fill in missing data with reasoned assumptions while minimizing potential biases that could arise from more arbitrary imputations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfVTMMpwm1PP"
      },
      "outputs": [],
      "source": [
        "weather_mode = filtered_df['Weather'].mode()[0]\n",
        "filtered_df['Weather'] = filtered_df['Weather'].fillna(weather_mode)\n",
        "\n",
        "filtered_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viJbGs-wTTg_"
      },
      "outputs": [],
      "source": [
        "collision_type_by_weather = filtered_df.groupby('Weather')['Collision Type'].apply(lambda x: x.mode()[0] if not x.mode().empty else np.nan).to_dict()\n",
        "\n",
        "def impute_collision_type(row):\n",
        "    if pd.isna(row['Collision Type']):\n",
        "        return collision_type_by_weather.get(row['Weather'], np.nan)\n",
        "    else:\n",
        "        return row['Collision Type']\n",
        "\n",
        "filtered_df['Collision Type'] = filtered_df.apply(impute_collision_type, axis=1)\n",
        "\n",
        "filtered_df['Collision Type'] = filtered_df['Collision Type'].fillna('Unknown')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb_OW3EtU9xn"
      },
      "source": [
        "The code determines the most common \"Collision Type\" for each \"Weather\" condition by grouping the data by \"Weather\" and calculating the mode of \"Collision Type\" within each group. This mapping of \"Weather\" conditions to the most frequent \"Collision Type\" is stored as a dictionary. Next, the user defines a function that checks if \"Collision Type\" is missing for a row; if it is, the function attempts to impute the missing value using the \"Weather\"-to-\"Collision Type\" mapping. This function is applied to each row of the DataFrame. If \"Collision Type\" is missing and the row's \"Weather\" is found in the mapping, the missing \"Collision Type\" is imputed with the most common type associated with that weather condition. In the end, if still Collision types are missing then they are replaced with the \"Unkown\" tag to remove bias from the table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7bW4zNLVguB"
      },
      "outputs": [],
      "source": [
        "null_counts_corrected = filtered_df.isnull().sum()\n",
        "print(null_counts_corrected)\n",
        "print(\"No Cases with NULL values left! -> Data is somewhat cleansed :)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsGKElPJnCkj"
      },
      "source": [
        "--------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4U5aa8gIm7Xu"
      },
      "source": [
        "# **Data Exploration and Summary Statistics**\n",
        "\n",
        "This stage, often referred to as the Exploratory Analysis and Visualization phase of the data lifecycle, is where we dive deep into the dataset to uncover trends, test hypotheses, and draw insights. By utilizing statistical analysis and visual tools, we aim to validate our hypotheses and inform decision-making processes.\n",
        "\n",
        "### Hypothesis Testing:\n",
        "We have formulated several hypotheses to guide our analysis and ensure it addresses specific research questions about traffic incidents in Montgomery County. Here's how we are approaching each hypothesis:\n",
        "\n",
        "1. **Light Conditions and Hit/Run Incidents:**\n",
        "   - **Null Hypothesis (H0)**: There is no significant difference in the proportion of hit/run incidents between daylight and dark conditions.\n",
        "   - **Alternative Hypothesis (H1)**: The proportion of hit/run incidents differs significantly between daylight and dark conditions.\n",
        "   - **Analysis Approach**: We will use a Z-test for two proportions, as it is suitable for large sample sizes. This test will help us compare the rates of hit/run incidents under different lighting conditions.\n",
        "\n",
        "2. **Weather Conditions and Crash Incidents:**\n",
        "   - **Null Hypothesis (H0)**: There is no significant difference in the number of crash incidents during clear versus adverse weather conditions.\n",
        "   - **Alternative Hypothesis (H1)**: There is a significant difference in the number of crash incidents between clear and adverse weather conditions.\n",
        "   - **Analysis Approach**: Depending on data categorization and distribution, we might use a chi-square test for categorical data or a t-test for continuous data to compare crash incident rates across different weather conditions.\n",
        "\n",
        "3. **Time of Day and Frequency of Crashes:**\n",
        "   - **Null Hypothesis (H0)**: The time of day does not significantly affect the frequency of crash incidents.\n",
        "   - **Alternative Hypothesis (H1)**: There is a significant difference in the frequency of crash incidents between daytime and nighttime.\n",
        "   - **Analysis Approach**: A chi-square test will be employed if we categorize the data into daytime and nighttime incidents to assess the counts and determine if the time of day influences crash frequency.\n",
        "\n",
        "### Data Visualization:\n",
        "In addition to hypothesis testing, we are employing various data visualization techniques to illustrate and explore potential trends within the dataset. This involves:\n",
        "\n",
        "- **Plotting Key Trends**: We use graphs such as bar charts, line graphs, and scatter plots to visually represent the data, making it easier to identify patterns or anomalies.\n",
        "- **Frequency Analysis**: By analyzing the frequency of specific types of incidents, such as hit/run or specific collision types, we can identify which scenarios occur most frequently and may require additional focus or preventive measures.\n",
        "\n",
        "### Statistical Analysis:\n",
        "We perform detailed statistical analyses to provide a robust foundation for our findings. This includes:\n",
        "- **Descriptive Statistics**: To summarize the central tendencies and dispersion in our data.\n",
        "- **Inferential Statistics**: To draw conclusions about the larger population from our sample data, specifically testing our hypotheses under different conditions.\n",
        "\n",
        "By the end of this phase, we aim to have a clear understanding of the factors that influence traffic incidents in Montgomery County, guided by data-driven insights and rigorous statistical validation. This approach not only helps in affirming or refuting our initial hypotheses but also aids in identifying areas where traffic safety can be improved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaRnUNQFlaBe"
      },
      "source": [
        "### **1.   Hypothesis: Light(Headlights) Conditions and Hit/Run Incidents**\n",
        "\n",
        "*Null Hypothesis (H0): The proportion of hit/run incidents is the same during daylight and dark conditions.*\n",
        "\n",
        "*Alternative Hypothesis (H1): The proportion of hit/run incidents during daylight is different from that during dark conditions.*\n",
        "\n",
        "*Test to Use: A Z-test for two proportions would be appropriate if you have a large sample size, which is likely given that this is traffic incident data. This test will compare the proportion of hit/run incidents in daylight vs. dark conditions.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILx4d0TOlkF_",
        "outputId": "094fddf3-7524-4c41-e998-87bb227f1e72"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from statsmodels.stats.proportion import proportions_ztest\n",
        "\n",
        "crash_data = filtered_df.copy()\n",
        "crash_data.loc[:, 'Hit_Run_Binary'] = (crash_data['Hit/Run'] == 'Yes').astype(int)\n",
        "\n",
        "dark_condition_values = ['DARK LIGHTS ON', 'DARK NO LIGHTS', 'DUSK', 'DAWN', 'DARK -- UNKNOWN LIGHTING']\n",
        "\n",
        "daylight_hitrun_count = crash_data.loc[crash_data['Light'] == 'DAYLIGHT', 'Hit_Run_Binary'].sum()\n",
        "dark_hitrun_count = crash_data.loc[crash_data['Light'].isin(dark_condition_values), 'Hit_Run_Binary'].sum()\n",
        "\n",
        "daylight_count = (crash_data['Light'] == 'DAYLIGHT').sum()\n",
        "dark_count = crash_data['Light'].isin(dark_condition_values).sum()\n",
        "\n",
        "if daylight_count > 0 and dark_count > 0:\n",
        "    counts = np.array([daylight_hitrun_count, dark_hitrun_count])\n",
        "    nobs = np.array([daylight_count, dark_count])\n",
        "    stat, pval = proportions_ztest(counts, nobs)\n",
        "else:\n",
        "    stat, pval = None, None\n",
        "\n",
        "print(f\"Daylight Hit/Run Count: {daylight_hitrun_count}\")\n",
        "print(f\"Dark Hit/Run Count: {dark_hitrun_count}\")\n",
        "print(f\"Total Incidents in Daylight Conditions: {daylight_count}\")\n",
        "print(f\"Total Incidents in Dark Conditions: {dark_count}\")\n",
        "print(f\"Z-test Statistic: {stat}\")\n",
        "print(f\"P-value: {pval}\")\n",
        "print(\"\\n\")\n",
        "alpha = 0.05\n",
        "if pval is not None:\n",
        "    if pval < alpha:\n",
        "        print(\"Reject the null hypothesis. The proportion of hit-and-run incidents is significantly different between daylight and dark conditions.\")\n",
        "    else:\n",
        "        print(\"Fail to reject the null hypothesis. There is not enough evidence to conclude that the proportion of hit-and-run incidents is significantly different between daylight and dark conditions.\")\n",
        "else:\n",
        "    print(\"Unable to perform the hypothesis test due to insufficient data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBMFeWm55Y2g"
      },
      "source": [
        "### **Conclusion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96FYZNMAp4TJ"
      },
      "source": [
        "The negative Z-test statistic indicates that the proportion of hit/run incidents in daylight conditions is lower than in dark conditions, and the extremely small p-value suggests that the difference is statistically significant. This means we reject the null hypothesis and accept the alternative hypothesis that the proportion of hit/run incidents during daylight is different from that during dark conditions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtP-1AppqtZy"
      },
      "source": [
        "### **Graph**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "1qD2o0Udqs3a",
        "outputId": "de3a954d-bbbd-4c0b-b878-f8a6d6930ab8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "daylight_proportion = daylight_hitrun_count / daylight_count\n",
        "dark_proportion = dark_hitrun_count / dark_count\n",
        "\n",
        "conditions = ['Daylight', 'Dark']\n",
        "proportions = [daylight_proportion, dark_proportion]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(conditions, proportions, color = 'pink')\n",
        "plt.xlabel('Lighting Conditions')\n",
        "plt.ylabel('Proportion of Hit-and-Run Incidents')\n",
        "plt.title('Proportion of Hit-and-Run Incidents by Lighting Conditions')\n",
        "\n",
        "for i, proportion in enumerate(proportions):\n",
        "    plt.text(i, proportion, f'{proportion:.3f}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64e6eR5jwC1m"
      },
      "source": [
        "### **2.   Hypothesis: Weather Conditions and Crash Incidents**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYdo1-RT3k3R"
      },
      "source": [
        "*Null Hypothesis (H0)*: There is no significant difference in the number of crash incidents during clear weather conditions and during adverse weather conditions (e.g., rain, snow, fog).\n",
        "\n",
        "⁠*Alternative Hypothesis (H1)*: There is a significant difference in the number of crash incidents between clear weather conditions and adverse weather conditions.\n",
        "\n",
        "*Test to Use*: We use a chi-square test if we categorize days as either having clear or adverse weather and count the number of crash incidents on such days. For continuous data (e.g., number of incidents), a t-test could be applicable, for the data distribution.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBORZdggwHQe",
        "outputId": "52d34ca7-2851-49cb-b949-f9494cf2fce2"
      },
      "outputs": [],
      "source": [
        "def categorize_weather(condition):\n",
        "    if condition == 'CLEAR':\n",
        "        return 'clear'\n",
        "    elif condition in ['UNKNOWN', 'OTHER']:\n",
        "        return 'exclude'\n",
        "    else:\n",
        "        return 'adverse'\n",
        "\n",
        "\n",
        "filtered_df = filtered_df.copy()\n",
        "filtered_df['Weather_Condition'] = filtered_df['Weather'].apply(categorize_weather)\n",
        "\n",
        "\n",
        "df_test2 = filtered_df[filtered_df['Weather_Condition'] != 'exclude']\n",
        "incident_counts_clear = df_test2[df_test2['Weather_Condition'] == 'clear'].groupby('Crash Date/Time').size()\n",
        "incident_counts_adverse = df_test2[df_test2['Weather_Condition'] == 'adverse'].groupby('Crash Date/Time').size()\n",
        "\n",
        "print(f\"incident_counts_clear: {incident_counts_clear.count()}, incident_counts_adverse: {incident_counts_adverse.count()}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "t_stat, p_val = ttest_ind(incident_counts_clear, incident_counts_adverse, equal_var=False, nan_policy='omit')\n",
        "\n",
        "\n",
        "print(f\"T-statistic: {t_stat}\")\n",
        "print(f\"P-value: {p_val}\")\n",
        "\n",
        "\n",
        "alpha = 0.05\n",
        "if p_val is not None:\n",
        "    if p_val < alpha:\n",
        "        print(\"Reject the null hypothesis. The proportion of hit-and-run incidents is significantly different between daylight and dark conditions.\")\n",
        "    else:\n",
        "        print(\"Fail to reject the null hypothesis. There is not enough evidence to conclude that the proportion of hit-and-run incidents is significantly different between daylight and dark conditions.\")\n",
        "else:\n",
        "    print(\"Unable to perform the hypothesis test due to insufficient data.\")\n",
        "\n",
        "print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMak71vW5SHd"
      },
      "source": [
        "### **Conclusion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XW-63aep2AOF"
      },
      "source": [
        "*  Based on the provided P-value of approximately 0.087, we would fail to reject the null hypothesis at a significance level of 0.05. This suggests that there is insufficient evidence to conclude a significant correlation between weather conditions and the number of crash incidents.\n",
        "\n",
        "*  However, it's important to note that the P-value is close to the significance threshold, indicating a marginal level of significance. Therefore, while we do not have strong evidence to support a correlation between weather conditions and crash incidents, further investigation with a larger dataset or alternative analytical approaches may be warranted to confirm this finding conclusively.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwb4gERD2X4t"
      },
      "source": [
        "### **Graph**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "qduBhT1w2iEj",
        "outputId": "dec5969f-2619-4eae-e234-a9d61fb8d842"
      },
      "outputs": [],
      "source": [
        "incident_counts = df_test2['Weather_Condition'].value_counts()\n",
        "sns.barplot(x=incident_counts.index, y=incident_counts.values, color = 'yellow')\n",
        "plt.title('Number of Crash Incidents by Weather Condition')\n",
        "plt.ylabel('Number of Incidents')\n",
        "plt.xlabel('Weather Condition')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6n3P8Fp3NlK"
      },
      "source": [
        "### **3.   Hypothesis: Time of Day and Crash Frequency**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZ46250b3h29"
      },
      "source": [
        "⁠*Null Hypothesis (H0)*: The time of day (daytime vs. nighttime) does not significantly affect the frequency of crash incidents.\n",
        "\n",
        "⁠*Alternative Hypothesis (H1)*: There is a significant difference in the frequency of crash incidents between daytime and nighttime.\n",
        "\n",
        "⁠*Test to Use*: This hypothesis can be tested using a chi-square test if we're comparing the count of incidents during daytime vs. nighttime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMdfPMKu5NOF",
        "outputId": "f597809f-b00c-4ade-d679-120f75de4f40"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "def categorize_time_of_day(time):\n",
        "    if 6 <= time.hour < 18:\n",
        "        return 'Day'\n",
        "    else:\n",
        "        return 'Night'\n",
        "\n",
        "def hypothesis_testing(df):\n",
        "    df = df.copy()\n",
        "\n",
        "    df['Crash Date/Time'] = pd.to_datetime(df['Crash Date/Time'])\n",
        "\n",
        "    df['Time of Day'] = df['Crash Date/Time'].apply(lambda x: categorize_time_of_day(x))\n",
        "\n",
        "    contingency_table = pd.crosstab(df['Time of Day'], df['Report Number'])\n",
        "\n",
        "    chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "    print(\"Chi-square statistic:\", chi2_stat)\n",
        "    print(\"P-value:\", p_val)\n",
        "    alpha = 0.05\n",
        "    if p_val is not None:\n",
        "        if p_val < alpha:\n",
        "            print(\"Reject the null hypothesis. The proportion of hit-and-run incidents is significantly different between daylight and dark conditions.\")\n",
        "        else:\n",
        "            print(\"Fail to reject the null hypothesis. There is not enough evidence to conclude that the proportion of hit-and-run incidents is significantly different between daylight and dark conditions.\")\n",
        "    else:\n",
        "        print(\"Unable to perform the hypothesis test due to insufficient data.\")\n",
        "\n",
        "    print(\"\\n\")\n",
        "\n",
        "hypothesis_testing(filtered_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Conclusion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "1.   With a p-value of approximately 0.498, which is greater than the typical significance level of 0.05, we fail to reject the null hypothesis.\n",
        "2.   There is insufficient evidence to suggest that the time of day (daytime vs. nighttime) significantly affects the frequency of crash incidents. Therefore, we do not have statistically significant support for the alternative hypothesis. It appears that there is no significant difference in the frequency of crash incidents between daytime and nighttime based on the available data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qokwbvh6B6hY"
      },
      "source": [
        "### **Graph**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "9Ao9vDTVB9H0",
        "outputId": "28ccef6d-9524-4837-b31a-b7615ec094de"
      },
      "outputs": [],
      "source": [
        "def make_graph(df):\n",
        "    sns.countplot(data=df, x='Time of Day', color = 'orange')\n",
        "    plt.title('Frequency of Crash Incidents by Time of Day')\n",
        "    plt.xlabel('Time of Day')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.show()\n",
        "\n",
        "make_graph(filtered_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **ML Algorithm Design/Development**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepare the target and features for the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['Hit/Run'] = (df['Hit/Run'] == 'Yes').astype(int)\n",
        "y = df['Hit/Run']\n",
        "X = df.drop('Hit/Run', axis=1)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **ML Algorithm Training and Test Data Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train a RandomForest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Predict on the testing set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **PREDICTION MODEL THAT PREDICTS HIT/RUN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preliminary data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert 'Crash Date/Time' to datetime and extract relevant parts\n",
        "df['Crash Date/Time'] = pd.to_datetime(df['Crash Date/Time'])\n",
        "df['Hour'] = df['Crash Date/Time'].dt.hour\n",
        "df['Month'] = df['Crash Date/Time'].dt.month\n",
        "df['DayOfWeek'] = df['Crash Date/Time'].dt.weekday"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assume these columns are relevant based on the description\n",
        "relevant_columns = ['Weather', 'Light', 'Traffic Control', 'Road Condition', 'Surface Condition', 'Hour', 'Month', 'DayOfWeek', 'Hit/Run','Driver Substance Abuse']\n",
        "\n",
        "# Filter the dataset\n",
        "df = df[relevant_columns]\n",
        "\n",
        "# Handle missing values\n",
        "imputer = SimpleImputer(strategy='most_frequent')\n",
        "df = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
        "\n",
        "# Encode categorical variables\n",
        "categorical_features = ['Weather', 'Light', 'Traffic Control', 'Road Condition', 'Surface Condition', 'DayOfWeek', 'Month', 'Driver Substance Abuse']\n",
        "df = pd.get_dummies(df, columns=categorical_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " # **Machine Learning Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " # **Post Result Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Visualization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Result**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Conclusion**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
